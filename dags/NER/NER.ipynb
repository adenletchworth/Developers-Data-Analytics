{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "from typing import List, Dict, Union, Generator, Optional\n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/adenletchworth/Downloads/studious-sign-417501-b1d1c2fe9312.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Query Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_bigquery_batched(query: str, parameters: Optional[List[bigquery.ScalarQueryParameter]] = None, batch_size: int = 1000) -> Generator[List[Union[tuple, Dict[str, any]]], None, None]:\n",
    "    \"\"\"\n",
    "    Streams results from a BigQuery SQL query in batches.\n",
    "\n",
    "    :param query: SQL query string to execute.\n",
    "    :param parameters: List of bigquery.ScalarQueryParameter objects for query parameterization.\n",
    "    :param batch_size: Number of results to fetch per batch.\n",
    "    :yield: Batches of results, each as a list of tuples or dictionaries.\n",
    "    \"\"\"\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Only set query_parameters in job_config if parameters are not None\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    if parameters:\n",
    "        job_config.query_parameters = parameters\n",
    "\n",
    "    try:\n",
    "        # Execute the query\n",
    "        query_job = client.query(query, job_config=job_config)\n",
    "\n",
    "        # Iterate over pages of the query results\n",
    "        for page in query_job.result(page_size=batch_size).pages:\n",
    "            batch = [(row.title, row.body) for row in page]\n",
    "            yield batch\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during query execution: {e}\")\n",
    "        yield []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotated Data for Training Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_annotate_with_phrase_matcher_batched(batches: Generator[List[Union[tuple, Dict[str, any]]], None, None]):\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "    titles_seen = set()\n",
    "\n",
    "    for batch in batches:\n",
    "        for title, body in batch:\n",
    "            normalized_title = title.lower()\n",
    "            if normalized_title not in titles_seen:\n",
    "                pattern = nlp.make_doc(normalized_title)\n",
    "                matcher.add(\"TECH_TERM\", [pattern])\n",
    "                titles_seen.add(normalized_title)\n",
    "            \n",
    "        annotated_batch = []\n",
    "        for title, body in batch:\n",
    "            normalized_title = title.lower()\n",
    "            modified_body = normalized_title + \". \" + body.lower()\n",
    "            doc = nlp(modified_body)\n",
    "\n",
    "            matches = matcher(doc)\n",
    "            entities = []\n",
    "            for match_id, start, end in matches:\n",
    "                span = doc[start:end]\n",
    "                entities.append((span.start_char, span.end_char, \"TECH_TERM\"))\n",
    "\n",
    "            annotated_batch.append((modified_body, {\"entities\": entities}))\n",
    "        \n",
    "        yield annotated_batch\n",
    "        \n",
    "def save_annotated_data_as_json(annotated_data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(annotated_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for Word2Vec Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_for_word2vec(batches):\n",
    "    annotated_batch = []\n",
    "    for batch in batches:\n",
    "        for body in batch:\n",
    "            annotated_batch.append(body)\n",
    "    return annotated_batch\n",
    "    \n",
    "        \n",
    "def create_json_for_spacy():\n",
    "    query = \"\"\"\n",
    "    SELECT title, body FROM `stack_overflow.posts_tag_wiki_excerpt`\n",
    "    WHERE title IS NOT NULL AND body IS NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = 10000  \n",
    "    batches = query_bigquery_batched(query, batch_size=batch_size)\n",
    "\n",
    "    all_annotated_data = []\n",
    "\n",
    "    for annotated_batch in normalize_and_annotate_with_phrase_matcher_batched(batches):\n",
    "        all_annotated_data.extend(annotated_batch) \n",
    "\n",
    "    file_path = \"./NER/CS_ENTITIES.json\"\n",
    "    save_annotated_data_as_json(all_annotated_data, file_path)\n",
    "\n",
    "\n",
    "def create_txt_for_word2vec(file_path):\n",
    "    query = \"\"\"\n",
    "    SELECT title,body FROM `stack_overflow.posts_tag_wiki_excerpt`\n",
    "    WHERE body IS NOT NULL\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = 10000  \n",
    "    batches = query_bigquery_batched(query, batch_size=batch_size)\n",
    "\n",
    "    all_annotated_data = create_data_for_word2vec(batches)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for _, sentence in all_annotated_data:\n",
    "            file.write(sentence + '\\n')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model_name):\n",
    "    with open('./data/word2vec.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extracting just the tokenized sentences from the JSON data\n",
    "    sentences = [item['tokenized'] for item in data]\n",
    "    \n",
    "    # Initialize the Word2Vec model\n",
    "    word2vec_model = Word2Vec(vector_size=300, window=5, min_count=1, workers=multiprocessing.cpu_count()-1)\n",
    "    \n",
    "    # Building vocabulary from the sentences\n",
    "    word2vec_model.build_vocab(sentences)\n",
    "    \n",
    "    # Training the Word2Vec model\n",
    "    word2vec_model.train(sentences, total_examples=word2vec_model.corpus_count, epochs=30)\n",
    "    \n",
    "    # Saving the trained model\n",
    "    word2vec_model.save(f\"./models/{model_name}.model\")\n",
    "    word2vec_model.wv.save_word2vec_format(f'./models/{model_name}.txt')\n",
    "\n",
    "\n",
    "training(\"word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(word):\n",
    "    model = KeyedVectors.load_word2vec_format('./models/word2vec_model.txt')\n",
    "    return model.most_similar(positive=[word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tensorflow', 0.7090263962745667), ('chainer', 0.6475980877876282), ('keras', 0.646369218826294), ('mxnet', 0.6445105671882629), ('tensor', 0.5889434814453125), ('xgboost', 0.563758909702301), ('tf', 0.5615367293357849), ('numpy', 0.5572689175605774), ('caffe', 0.5471710562705994), ('torch', 0.5371536612510681)]\n"
     ]
    }
   ],
   "source": [
    "print(get_similarity(\"pytorch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format('./models/word2vec_model.txt', binary=False)  \n",
    "\n",
    "for word in word2vec_model.key_to_index.keys():\n",
    "    vector = word2vec_model.get_vector(word)\n",
    "    nlp.vocab.set_vector(word, vector)\n",
    "\n",
    "\n",
    "nlp.to_disk('./models/spacy_word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow: 0.7090263712129975\n",
      "chainer: 0.6475980879456193\n",
      "keras: 0.6463692944339569\n",
      "mxnet: 0.644510513292877\n",
      "tensor: 0.5889435285062421\n"
     ]
    }
   ],
   "source": [
    "# Load the updated spaCy model\n",
    "nlp = spacy.load('./models/spacy_word2vec_model')\n",
    "\n",
    "def most_similar(word, topn=10):\n",
    "    queried_token = nlp.vocab[word]\n",
    "    \n",
    "    # Ensure the word exists in the vocabulary\n",
    "    if not queried_token.has_vector:\n",
    "        print(f\"The word '{word}' does not exist in the model's vocabulary.\")\n",
    "        return []\n",
    "    \n",
    "    # Calculate cosine similarity between the queried word's vector and all other vectors\n",
    "    similarities = []\n",
    "    for key, vector in nlp.vocab.vectors.items():\n",
    "        if nlp.vocab.strings[key] != word:  # exclude the queried word itself\n",
    "            similarity = 1 - spatial.distance.cosine(queried_token.vector, vector)\n",
    "            similarities.append((nlp.vocab.strings[key], similarity))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    most_similar = sorted(similarities, key=lambda item: item[1], reverse=True)[:topn]\n",
    "    \n",
    "    return most_similar\n",
    "\n",
    "word = 'pytorch'  \n",
    "similar_words = most_similar(word, topn=5)\n",
    "for similar_word, similarity in similar_words:\n",
    "    print(f\"{similar_word}: {similarity}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
